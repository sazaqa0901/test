{ " #alexnet, lr=5e-5 " }
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOlLQDGCfsRFvoBWemq+Wzk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sazaqa0901/test/blob/main/test1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GL09-Ub-9e6A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "j5dNavOw9idv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a213ba0d-80de-4b47-bac7-51deaf071dcf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResizeWithPad:\n",
        "    def __init__(self, target_size):\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        w, h = img.size\n",
        "\n",
        "        scale = self.target_size / max(w, h)\n",
        "\n",
        "        new_w = int(w * scale)\n",
        "        new_h = int(h * scale)\n",
        "\n",
        "        # 비율 유지하며 리사이즈\n",
        "        resized_img = img.resize((new_w, new_h), Image.LANCZOS)\n",
        "\n",
        "        # 검은색 캔버스 생성\n",
        "        canvas = Image.new(\"RGB\", (self.target_size, self.target_size), (0, 0, 0))\n",
        "\n",
        "        # 캔버스 중앙에 리사이즈된 이미지 배치\n",
        "        pad_x = (self.target_size - new_w) // 2\n",
        "        pad_y = (self.target_size - new_h) // 2\n",
        "\n",
        "        canvas.paste(resized_img, (pad_x, pad_y))\n",
        "\n",
        "        return canvas"
      ],
      "metadata": {
        "id": "I-6_tqLl9igO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform=None):\n",
        "        self.resizer = ResizeWithPad(224)\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 디스크에서 이미지 경로로 이미지 로드 (PIL)\n",
        "        img_path = self.paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        image = self.resizer(image)\n",
        "\n",
        "\n",
        "        return image"
      ],
      "metadata": {
        "id": "qDuryj259ilG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1c347645"
      },
      "outputs": [],
      "source": [
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "class RAMDataset(Dataset):\n",
        "    def __init__(self, images_uint8, labels, transform=None):\n",
        "        self.images = images_uint8 # (N, C, H, W) uint8 tensor\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        # 정규화용 transform (여기서 float변환 및 정규화 수행)\n",
        "        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    def __getitem__(self, idx):\n",
        "        # 1. uint8 텐서 가져오기\n",
        "        img = self.images[idx]\n",
        "\n",
        "        # 2. float32로 변환 및 0~1 스케일링 (매우 빠름)\n",
        "        img = img.float() / 255.0\n",
        "\n",
        "        # 3. 정규화 적용\n",
        "        img = self.normalize(img)\n",
        "\n",
        "        # 4. 추가 증강(Augmentation)이 있다면 여기서 적용 가능\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return img, label\n",
        "\n",
        "def load_images_to_ram_uint8(paths):\n",
        "    \"\"\"\n",
        "    이미지를 읽어서 리사이즈 후 uint8 텐서로 변환하여 리스트에 저장\n",
        "    \"\"\"\n",
        "    to_tensor = transforms.PILToTensor() # uint8 유지 (0-255)\n",
        "    resizer = ResizeWithPad(224)\n",
        "\n",
        "    tensor_list = []\n",
        "    print(f\"Loading {len(paths)} images to RAM (uint8 mode)...\")\n",
        "\n",
        "    for path in tqdm(paths):\n",
        "        try:\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            img = resizer(img)\n",
        "            tensor = to_tensor(img) # (3, 224, 224) uint8 Tensor\n",
        "            tensor_list.append(tensor)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {path}: {e}\")\n",
        "\n",
        "\n",
        "    return torch.stack(tensor_list) # (N, 3, 224, 224) uint8 Tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNetLike(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(AlexNetLike, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Conv 1\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2), # 224 -> 55\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2), # 55 -> 27\n",
        "\n",
        "            # Conv 2\n",
        "            nn.Conv2d(96, 256, kernel_size=5, padding=2), # 27 -> 27\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2), # 27 -> 13\n",
        "\n",
        "            # Conv 3\n",
        "            nn.Conv2d(256, 384, kernel_size=3, padding=1), # 13 -> 13\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Conv 4\n",
        "            nn.Conv2d(384, 384, kernel_size=3, padding=1), # 13 -> 13\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Conv 5\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1), # 13 -> 13\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2), # 13 -> 6\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 6 * 6, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, num_classes) # num_classes=1\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x); x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1); x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WFqz2Y8P9iuf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model_name, device):\n",
        "\n",
        "    model = None\n",
        "    num_classes = 1 # 이진 분류 (Real/Fake)\n",
        "\n",
        "    print(f\"Loading {model_name} architecture (FROM SCRATCH)...\")\n",
        "\n",
        "    if model_name.lower() == 'alexnet':\n",
        "        # 직접 짠 AlexNet (Conv 5, FC 3)\n",
        "        model =  AlexNetLike(num_classes=num_classes)\n",
        "\n",
        "    elif model_name.lower() == 'vgg16':\n",
        "        # VGG16\n",
        "        model = models.vgg16(weights=None, num_classes=num_classes)\n",
        "\n",
        "    elif model_name.lower() == 'googlenet':\n",
        "        # GoogLeNet\n",
        "        model = models.googlenet(weights=None, num_classes=num_classes, aux_logits=False)\n",
        "\n",
        "    elif model_name.lower() == 'resnet50':\n",
        "        # ResNet50\n",
        "        model = models.resnet50(weights=None, num_classes=num_classes)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model name: {model_name}. Choose from 'alexnet, 'vgg16', 'googlenet', 'resnet50'\")\n",
        "\n",
        "    return model.to(device)"
      ],
      "metadata": {
        "id": "opsguJzt9izn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1989208d"
      },
      "outputs": [],
      "source": [
        "# 학습\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs, patience):\n",
        "    print(\"=== 학습 시작 ===\")\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_weights = None\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # --- 훈련 ---\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        # tqdm으로 진행 상황 표시\n",
        "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
        "\n",
        "        for images, labels in train_pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # 순전파\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # 역전파 및 최적화\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # 정확도 계산\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (preds == labels).sum().item()\n",
        "\n",
        "            train_pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = correct_train / total_train\n",
        "\n",
        "        # --- 검증 ---\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
        "            for images, labels in val_pbar:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item() * images.size(0)\n",
        "\n",
        "                preds = torch.sigmoid(outputs) > 0.5\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (preds == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        epoch_val_acc = correct_val / total_val\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - {elapsed_time:.0f}s - \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} - \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "        # --- Early Stopping 및 Best Model 저장 ---\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            print(f\"  Validation loss decreased ({best_val_loss:.4f} --> {epoch_val_loss:.4f}). Saving model...\")\n",
        "            best_val_loss = epoch_val_loss\n",
        "            best_model_weights = copy.deepcopy(model.state_dict())\n",
        "            torch.save(best_model_weights, MODEL_SAVE_PATH)\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"  Validation loss did not improve. Patience: {epochs_no_improve}/{patience}\")\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "            break\n",
        "\n",
        "    print(\"=== 학습 완료 ===\")\n",
        "    model.load_state_dict(best_model_weights)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_test_loss = 0.0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "\n",
        "    print(\"\\n=== 테스트셋 평가 시작 ===\")\n",
        "    with torch.no_grad():\n",
        "        test_pbar = tqdm(test_loader, desc=\"[Test]\", leave=False)\n",
        "        for images, labels in test_pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_test_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (preds == labels).sum().item()\n",
        "\n",
        "    test_loss = running_test_loss / len(test_loader.dataset)\n",
        "    test_acc = correct_test / total_test\n",
        "\n",
        "    print(f\"===== 최종 테스트 결과 =====\")\n",
        "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"  Test Accuracy: {test_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "9bPDf2Ww9i3m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_file_name = '/content/drive/MyDrive/Dataset.zip'\n",
        "extraction_dir = '/content/dataset'\n",
        "with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extraction_dir)"
      ],
      "metadata": {
        "id": "j92-6h_N9i56"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 64\n",
        "NUM_SAMPLES = 45000\n",
        "LEARNING_RATE = 5e-5\n",
        "PATIENCE = 5"
      ],
      "metadata": {
        "id": "vXSR9lRr-BUV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 메인 실행\n",
        "start_time = time.time()\n",
        "\n",
        "# GPU 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# 데이터 경로 및 라벨 수집\n",
        "Fake_PATH = \"/content/dataset/Dataset/Train/Fake\"\n",
        "Real_PATH = \"/content/dataset/Dataset/Train/Real\"\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/ML/deepfake_baseline_model.pth\"\n",
        "print(\"이미지 경로 수집 중...\")\n",
        "face_real_dir = os.path.join(Real_PATH)\n",
        "face_fake_dir = os.path.join(Fake_PATH)\n",
        "\n",
        "real_paths = glob.glob(os.path.join(face_real_dir, \"*.*\"))\n",
        "fake_paths = glob.glob(os.path.join(face_fake_dir, \"*.*\"))\n",
        "\n",
        "all_paths = real_paths + fake_paths\n",
        "all_labels = [0] * len(real_paths) + [1] * len(fake_paths)\n",
        "\n",
        "print(f\"총 {len(all_labels)}개 이미지 경로 발견.\")\n",
        "\n",
        "\n",
        "#5만개 샘플링\n",
        "print(f\"{NUM_SAMPLES}개 샘플을 샘플링...\")\n",
        "_, target_paths, _, target_labels = train_test_split(\n",
        "    all_paths, all_labels,\n",
        "    test_size= NUM_SAMPLES,\n",
        "    random_state=42,\n",
        "    stratify=all_labels\n",
        ")\n",
        "print(f\"샘플링 완료: {NUM_SAMPLES}개 이미지 선택.\")"
      ],
      "metadata": {
        "id": "TQdrB4Mh-BKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cbe1e8f-1a28-4d1f-b6d6-a954dfa5e181"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "이미지 경로 수집 중...\n",
            "총 140002개 이미지 경로 발견.\n",
            "45000개 샘플을 샘플링...\n",
            "샘플링 완료: 45000개 이미지 선택.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"데이터를 7:2:1 비율로 분할합니다...\")\n",
        "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "    target_paths, target_labels, test_size=0.3, random_state=42, stratify=target_labels\n",
        ")\n",
        "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "    temp_paths, temp_labels, test_size=(1/3), random_state=42, stratify=temp_labels\n",
        ")\n",
        "print(f\"분할 완료: Train {len(train_paths)}개, Validation {len(val_paths)}개, Test {len(test_paths)}개\")"
      ],
      "metadata": {
        "id": "CdMnbU6J-BIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d09946b-42be-4dbe-e476-807829f64c3e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터를 7:2:1 비율로 분할합니다...\n",
            "분할 완료: Train 31500개, Validation 9000개, Test 4500개\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 및 RAM에 로드\n",
        "print(\"데이터 전처리를 시작합니다 (모든 데이터를 RAM에 로드)...\")\n",
        "\n",
        "# 모든 데이터를 RAM으로 로드\n",
        "X_train = load_images_to_ram_uint8(train_paths)\n",
        "y_train = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "X_val = load_images_to_ram_uint8(val_paths)\n",
        "y_val = torch.tensor(val_labels, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "X_test = load_images_to_ram_uint8(test_paths)\n",
        "y_test = torch.tensor(test_labels, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "print(\"모든 데이터를 RAM에 로드 완료.\")\n",
        "\n",
        "# RAM 기반의 TensorDataset과 DataLoader 생성\n",
        "train_dataset = RAMDataset(X_train, y_train, transform=None)\n",
        "val_dataset = RAMDataset(X_val, y_val, transform=None)\n",
        "test_dataset = RAMDataset(X_test, y_test, transform=None)\n",
        "\n",
        "# RAM에서 읽으므로 num_workers=0, pin_memory=False (이미 RAM에 있음)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(\"데이터 로더 생성 완료.\")"
      ],
      "metadata": {
        "id": "b2bGm-Z--BFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78fb00a0-9dc7-4298-84ab-b1e10d586975"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 전처리를 시작합니다 (모든 데이터를 RAM에 로드)...\n",
            "Loading 31500 images to RAM (uint8 mode)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31500/31500 [01:51<00:00, 283.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 9000 images to RAM (uint8 mode)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9000/9000 [00:35<00:00, 250.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 4500 images to RAM (uint8 mode)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4500/4500 [00:17<00:00, 251.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모든 데이터를 RAM에 로드 완료.\n",
            "데이터 로더 생성 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "# 8. 모델, 손실함수, 옵티마이저 정의\n",
        "model = get_model('alexnet', device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# 9. 학습 및 평가\n",
        "model = train_model(model, train_loader, val_loader, criterion, optimizer, device, EPOCHS, PATIENCE)\n",
        "evaluate_model(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"총 실행 시간: {(time.time() - start_time) / 60:.2f} 분\")"
      ],
      "metadata": {
        "id": "aZnszUtP-BAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe637d1-9288-4be5-c0cb-44fc47f0efec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading alexnet architecture (FROM SCRATCH)...\n",
            "=== 학습 시작 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 - 68s - Train Loss: 0.4317, Train Acc: 0.7860 - Val Loss: 0.2564, Val Acc: 0.8927\n",
            "  Validation loss decreased (inf --> 0.2564). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/30 - 64s - Train Loss: 0.2075, Train Acc: 0.9138 - Val Loss: 0.2388, Val Acc: 0.9017\n",
            "  Validation loss decreased (0.2564 --> 0.2388). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/30 - 64s - Train Loss: 0.1439, Train Acc: 0.9423 - Val Loss: 0.1431, Val Acc: 0.9458\n",
            "  Validation loss decreased (0.2388 --> 0.1431). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/30 - 64s - Train Loss: 0.1117, Train Acc: 0.9561 - Val Loss: 0.2386, Val Acc: 0.9124\n",
            "  Validation loss did not improve. Patience: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/30 - 63s - Train Loss: 0.0912, Train Acc: 0.9644 - Val Loss: 0.1791, Val Acc: 0.9368\n",
            "  Validation loss did not improve. Patience: 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/30 - 63s - Train Loss: 0.0737, Train Acc: 0.9707 - Val Loss: 0.1341, Val Acc: 0.9459\n",
            "  Validation loss decreased (0.1431 --> 0.1341). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/30 - 64s - Train Loss: 0.0606, Train Acc: 0.9759 - Val Loss: 0.2481, Val Acc: 0.9289\n",
            "  Validation loss did not improve. Patience: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/30 - 63s - Train Loss: 0.0555, Train Acc: 0.9770 - Val Loss: 0.1699, Val Acc: 0.9453\n",
            "  Validation loss did not improve. Patience: 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/30 - 63s - Train Loss: 0.0510, Train Acc: 0.9797 - Val Loss: 0.1441, Val Acc: 0.9517\n",
            "  Validation loss did not improve. Patience: 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/30 - 63s - Train Loss: 0.0415, Train Acc: 0.9829 - Val Loss: 0.1759, Val Acc: 0.9466\n",
            "  Validation loss did not improve. Patience: 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/30 - 64s - Train Loss: 0.0411, Train Acc: 0.9841 - Val Loss: 0.1598, Val Acc: 0.9534\n",
            "  Validation loss did not improve. Patience: 5/5\n",
            "Early stopping triggered after 11 epochs.\n",
            "=== 학습 완료 ===\n",
            "\n",
            "=== 테스트셋 평가 시작 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== 최종 테스트 결과 =====\n",
            "  Test Loss: 0.1351\n",
            "  Test Accuracy: 94.64%\n",
            "총 실행 시간: 11.93 분\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}
